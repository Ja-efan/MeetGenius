SYSTEM_OS=linux  # linux, macos, windows 중 하나

DJANGO_URL=  # Django STT Send URL

# 모든 상대 경로는 uvicorn app 실행하는 디렉토리 기준으로 작성 -> S12P11B203/Jetson/fastapi/

################################## CHROMADB CONFIG ##################################
CHROMADB_PERSISTENT_CLIENT_PATH ="../chromaDB/data"  # ChromaDB Persistent Client Path

CHROMADB_HTTP_CLIENT_HOST = "chromadb-server"  # ChromaDB HTTP Client Host (docker-compose.yml 에서 설정한 이름)
CHROMADB_HTTP_CLIENT_PORT = 8001  # ChromaDB HTTP Client Port
CHROMADB_HTTP_CLIENT_SSL = false  # ChromaDB HTTP Client SSL

################################## LLM MODEL CACHES CONFIG ##################################
LLM_MODEL_CACHES_DIR = "./.llm-model-caches/"  # LLM 모델 캐시 디렉토리
HUGGINGFACE_CACHE_DIR = "./.llm-model-caches/huggingface-caches/"  # HuggingFace 캐시 디렉토리
LLAMACPP_CACHE_DIR = "./.llm-model-caches/llamacpp-cache/"  # LlamaCPP 캐시 디렉토리


################################## EMBEDDING MODEL CONFIG ##################################
EMBEDDING_MODEL_NAME = "nlpai-lab/KoE5"  # 임베딩 모델 이름
EMBEDDING_MODEL_LOW_CPU_USAGE = true  # 임베딩 모델 저사양 사용 여부

################################## EMBEDDING MODEL QUANTIZATION CONFIG ##################################
EMBEDDING_MODEL_LOAD_IN_4BIT = True  # 4-bit 양자화 사용 여부
EMBEDDING_MODEL_BBN_4BIT_COMPUTE_DTYPE = torch.float16  # 4-bit 양자화 사용 시 사용할 데이터 타입
# EMBEDDING_MODEL_LOAD_IN_8BIT = False # 8-bit 양자화 사용 여부
# EMBEDDING_MODEL_BBN_8BIT_COMPUTE_DTYPE = torch.float16  # 8-bit 양자화 사용 시 사용할 데이터 타입


################################## RAG MODEL CONFIG ##################################
RAG_MODEL_NAME = "EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf"  # RAG 모델 이름
RAG_MODEL_N_CTX = 2048  # 모델 입력 프롬프트 최대 토큰 수
RAG_MODEL_TEMPERATURE = 0.0  # 모델 온도 (텍스트 생성 시 선택하는 단어의 무작위성을 조절해 결과의 다양성을 결정, 0으로 갈수록 무작위성 감소)
RAG_MODEL_N_GPU_LAYERS = -1  # 모델 GPU 레이어 수 (-1: 모든 레이어 사용)

################################## SUMMARY MODEL CONFIG ##################################
SUMMARY_MODEL_NAME = "EXAONE-3.5-2.4B-Instruct-Q6_K_L.gguf"  # SUMMARY 모델 이름
SUMMARY_MODEL_N_CTX = 4096  # 모델 입력 프롬프트 최대 토큰 수
SUMMARY_MODEL_TEMPERATURE = 0.0  # 모델 온도 (텍스트 생성 시 선택하는 단어의 무작위성을 조절해 결과의 다양성을 결정, 0으로 갈수록 무작위성 감소)
SUMMARY_MODEL_N_GPU_LAYERS = -1  # 모델 GPU 레이어 수 (-1: 모든 레이어 사용)
