import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig, PreTrainedTokenizerFast, BartForConditionalGeneration
from sentence_transformers import SentenceTransformer, models
from llama_cpp import Llama
from fastapi import FastAPI
from pathlib import Path
from app.utils import logging_config

# ë¡œê¹… ì„¤ì •
logger = logging_config.setup_logger(__name__)

# ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
BASE_DIR = Path(__file__).resolve().parent.parent.parent
LLM_MODELS_DIR = BASE_DIR / ".llm-model-caches"


def load_stt_model(app: FastAPI):
    """
    STT ëª¨ë¸ì„ ë¡œë“œ í›„ ë°˜í™˜í™˜
    """

    if not hasattr(app.state, "stt_model"):
        logger.info(f"ğŸ”„ [INFO] Loading STT model ...")
        stt_model = None  # STT ëª¨ë¸ ë¡œë“œ 
        logger.info(f"âœ… [INFO] STT model loaded successfully!") 
        
        return stt_model


def load_embedding_model(app: FastAPI):
    """
    Embedding ëª¨ë¸ ë¡œë“œ í›„ ë°˜í™˜í™˜
    """
    if not hasattr(app.state, "embedding_model"):

        model_name_or_path ="nlpai-lab/KoE5"
        
        logger.info(f"ğŸ”„ [INFO] Loading Embedding model ...")

        # ì–‘ìí™” ì„¤ì • 
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True, # 4-bit ì–‘ìí™” 
            # load_in_8bit=True, # 8-bit ì–‘ìí™”
        )

        # ì–‘ìí™” ëª¨ë¸ ë¡œë“œ 
        quantized_model = AutoModel.from_pretrained(model_name_or_path,
                                          quantization_config=quantization_config,
                                          cache_dir=LLM_MODELS_DIR / "huggingface-caches")
        # Tokenizer ë¡œë“œ 
        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

        # SentenceTransformer ëª¨ë“ˆ êµ¬ì„±
        word_embedding_model = models.Transformer(model_name_or_path)
        word_embedding_model.auto_model = quantized_model  # ê¸°ì¡´ ëª¨ë¸ì„ ì–‘ìí™”ëœ ëª¨ë¸ë¡œ êµì²´ 

        # Pooling ë ˆì´ì–´ ì¶”ê°€ (ë¬¸ì¥ ìˆ˜ì¤€ì˜ ì„ë² ë”©ì„ ìœ„í•´ í•„ìˆ˜)
        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())

        # SentenceTransformer ê°ì²´ ìƒì„± (Transformerì™€ Poolingì„ ê²°í•©)
        sentence_embedding_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

        # Tokenizerë„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
        sentence_embedding_model.tokenizer = tokenizer
        
        logger.info(f"âœ… [INFO] Embedding model loaded successfully!")
        
        return sentence_embedding_model


def load_rag_model(app: FastAPI, 
                   rag_model_name="EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf",
                   n_gpu_layers: int=-1,
                   metadata: bool=True,
                   context_params: bool=True):
    try:
        print(f"âœ… [INFO] Loading RAG model: {rag_model_name}...")
        llama_cpp_dir = LLM_MODELS_DIR / "llama-cpp"
        print(f"âœ… [INFO] LLM_MODELS_DIR: {llama_cpp_dir}")

        # ëª¨ë¸ ë¡œë“œ
        rag_model = Llama(
            model_path=str(llama_cpp_dir / rag_model_name), 
            n_ctx=2048,  # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° (ì…ë ¥ í”„ë¡¬í”„íŠ¸ í† í° ìµœëŒ€ 2048)
            n_gpu_layers=n_gpu_layers)

        logger.info(f"âœ… [INFO] RAG model successfully stored in app.state!")

        # # ë°˜í™˜í•  ë°ì´í„° êµ¬ì„±
        # response_data = {}
        
        # if metadata:
        #     response_data["metadata"] = app_state.rag_model.metadata
        
        # if context_params:
        #     response_data["context_params"] = app_state.rag_model.context_params
        
        # # ë°˜í™˜í•  ë°ì´í„°ê°€ ì—†ë‹¤ë©´ Noneì„ ë°˜í™˜
        # return response_data if response_data else None
        
        return rag_model

    except Exception as e:
        logger.error(f"âŒ [ERROR] Failed to load RAG model {rag_model_name}: {e}")
        raise e


def load_summary_model(app: FastAPI):
    """
    Summary ëª¨ë¸ ë¡œë“œ í›„ ë°˜í™˜
    """
    if not hasattr(app.state, "summary_model"):
        try:
            print(f"Loading summary model ...")

            # ìš”ì•½ ëª¨ë¸ ë¡œë”©
            model_name = 'gangyeolkim/kobart-korean-summarizer-v2'
            tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)
            model = BartForConditionalGeneration.from_pretrained(model_name)

            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ app.stateì— ì €ì¥
            app.state.summary_model = {
                'tokenizer': tokenizer,
                'model': model
            }

            logger.info("âœ… [INFO] Summary model loaded successfully!")

        except Exception as e:
            logger.error(f"âŒ [ERROR] Failed to load summary model: {str(e)}")
            return None
        
    return app.state.summary_model


def unload_models(app: FastAPI):
    """
    FastAPI ìƒíƒœ(app.state)ì—ì„œ ëª¨ë¸ì„ ì œê±°í•˜ì—¬ ë©”ëª¨ë¦¬ í•´ì œ 
    """
    if hasattr(app.state, "stt_model"):
        logger.info(f"ğŸ”„ [INFO] Unloading model: stt_model")
        del app.state.stt_model
        print(f"STT model unloaded!")

    if hasattr(app.state, "embedding_model"):
        logger.info(f"ğŸ”„ [INFO] Unloading model: embedding_model")
        del app.state.embedding_model
        print(f"Embedding model unloaded!")

    if hasattr(app.state, "rag_model"):
        logger.info(f"ğŸ”„ [INFO] Unloading model: rag_model")
        del app.state.rag_model
        print(f"RAG model unloaded!")
    
    if hasattr(app.state, "summary_model"):
        logger.info(f"ğŸ”„ [INFO] Unloading model: summary_model")
        del app.state.summary_model
        print(f"Summary model unloaded!")
    
    logger.info("âœ… [INFO] All models unloaded successfully!")
