"""
    RAG ê´€ë ¨ í•¨ìˆ˜ ëª¨ë“ˆ 
"""
import time
from fastapi import FastAPI, HTTPException
from app.utils import chromadb_utils, llm_utils

async def rag_process(query: str, app: FastAPI):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ì—¬ RAG ê¸°ë°˜ì˜ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ 

    Args:
        query (str): ì‚¬ìš©ì ì§ˆë¬¸
        app (FastAPI): FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ 
    
    Returns:
        str: RAG ì‘ë‹µ (ê²€ìƒ‰ëœ ë¬¸ì„œ ê¸°ë°˜ ìƒì„±)
    """

    
    # í•„ìš”í•œ ëª¨ë¸ê³¼ DBê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸ 
    if not hasattr(app.state, "project_collection"):
        print(f"ğŸ”„ [INFO] Project collection not found in app.state, loading...")
        app.state.project_collection = chromadb_utils.ProjectCollection(
            project_id=app.state.project_id,
            app=app
        )
        print(f"âœ… [INFO] Project collection loaded successfully!")
        
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ 
    if not hasattr(app.state, "embedding_model"):
        print(f"ğŸ”„ [INFO] Embedding model not found in app.state, loading...")
        app.state.embedding_model = llm_utils.load_embedding_model(app_state=app)
        
    # RAG ëª¨ë¸ ë¡œë“œ 
    if not hasattr(app.state, "rag_model"):
        print(f"ğŸ”„ [INFO] RAG model not found in app.state, loading...")
        app.state.rag_model = llm_utils.load_rag_model(app_state=app)
        
    # í•„ìš”í•œ ëª¨ë¸ê³¼ DB ê°€ì ¸ì˜¤ê¸° 
    project_collection = app.state.project_collection
    embedding_model = app.state.embedding_model
    rag_model = app.state.rag_model
    
    # ëª¨ë¸ê³¼ DBê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸ 
    if not project_collection or not embedding_model or not rag_model:
        raise HTTPException(status_code=500, detail="RAG ì‹œìŠ¤í…œì´ ì™„ì „íˆ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    # KoE5 ëª¨ë¸ ì„ë² ë”© í”„ë¡œì„¸ìŠ¤
    formatted_query = [f"query: {query}"]

    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜ 
    query_embedding = embedding_model.encode(formatted_query)

    # ChroaaDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (Top-K ê²€ìƒ‰)
    search_results = project_collection.search_documents(query_embedding=query_embedding, top_k=1)
    retrieved_content = search_results["documents"][0]
    retrieved_doc_ids = search_results["ids"][0]
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (EXAONE3.5)
    prompt = [

    ]
    
    start_time = time.time()
    result = rag_model(
        prompt,
        max_new_tokens=1000,  # ë‹µë³€ ìƒì„± ìµœëŒ€ í† í° ìˆ˜ ì œí•œ 
        temperature=0.2  # ë‹µë³€ ìƒì„± ì˜¨ë„ ì¡°ì ˆ 
    )
    end_time = time.time()
    print(f"ğŸ”„ [INFO] RAG ì‘ë‹µ ìƒì„± ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")

    # ë‹µë³€ í˜•ì‹ ì •ë¦¬ 
    answer = result["choices"][0]["text"]

    return {"answer": answer, "doc_ids": retrieved_doc_ids}

